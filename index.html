
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="seal_icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"/>
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Nan Jiang</title>
  <meta name="Nan Jiang's Homepage"http-equiv="Content-Type" content="Nan Jiang's Homepage">

  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src='https://www.google.com/recaptcha/api.js'></script>

</head>

<body>
  <div class="fixed-block">
    <h2 style="margin-left: 10%; display: inline-block;">Nan Jiang</h2>
    <p style="margin-left: 45%; display: inline-block;">
      <i class="fa fa-envelope">&nbsp; </i><a href="mailto:nan.jiang@stu.pku.edu.cn">Email</a> &nbsp; &nbsp; &nbsp;
      <i class="ai ai-google-scholar fa-xl">&nbsp; </i><a href="https://scholar.google.com/citations?user=xSYWn08AAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp; &nbsp; &nbsp; 
      <i class="fa fa-github">&nbsp; </i><a href="https://github.com/jnnan" target="_blank">Github</a> &nbsp; &nbsp; &nbsp;

      
    </p>
  </div>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Nan Jiang&emsp;|&emsp;蒋楠</name>
              </p>
              <p>I'm a Ph.D. student in Institute for AI, Peking University. I'm a member of the <a href="https://pku.ai/", target="_blank">CoRe Lab</a>, advised by Prof. <a href="https://yzhu.io/", target="_blank">Yixin Zhu</a>. I am currently an intern at Beijing Institute for General Artificial Intelligence (BIGAI). I received my M.Sc. from Department of Computing at Imperial College London.
              <br>
              <br>
              <br>
              <p style="text-align:center">
                <i class="fa fa-envelope">&nbsp; </i><a href="mailto:nan.jiang@stu.pku.edu.cn">Email</a> &nbsp; &nbsp; &nbsp;
                <i class="ai ai-google-scholar fa-xl">&nbsp; </i><a href="https://scholar.google.com/citations?user=xSYWn08AAAAJ&hl=en", target="_blank">Google Scholar</a> &nbsp; &nbsp; &nbsp; 
                <i class="fa fa-github">&nbsp; </i><a href="https://github.com/jnnan", target="_blank">Github</a> &nbsp; &nbsp; &nbsp;
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me/jiangnan.png"><img style="width:60%;max-width:60%" alt="profile photo" src="images/me/jiangnan.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        
        <!-- Research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Research</h2>
            My research interests lie in computer vision and graphics. I currently focus on the understanding of human-object interaction in real 3D scenes, human motion synthesis and humanoid control. My long-term research goal is to utilize insights from human behavior to create future conveniences, such as assistive robots.
              <br>
             
              
          </td></tr>
        </table>

        <br>
        <!-- Preprints -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Preprints</h2>
          </td></tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">

        </table>




        <!-- Publications -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Publications</h2>
          </td></tr>
        </table>


<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        </table>-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">

          <tr>
            <td width="33%" valign="top"><a href="images/lingo/teaser.webp"><img src="images/lingo/teaser.webp" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2410.03187", target="_blank" id="">
              <heading>Autonomous Character-Scene Interaction Synthesis from Text Instruction</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
                <strong>Nan Jiang*</strong>,
                <a href="https://pku.ai/author/zimo-he/", target="_blank">Zimo He*</a>,
                <a href="", target="_blank">Zi Wang</a>,
                <a href="https://pku.ai/author/hongjie-li/", target="_blank">Hongjie Li</a>,
                <a href="https://yixchen.github.io/", target="_blank">Yixin Chen</a>,
                <a href="https://siyuanhuang.com/", target="_blank">Siyuan Huang</a>,
                <a href="https://yzhu.io/", target="_blank">Yixin Zhu</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2024
              <br></p>
              <div class="paper" id="lingo">
                <a onmouseover="showblock('lingo_abs')" onmouseout="hideblock('lingo_abs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2410.03187" target="_blank">paper</a> /
<!--                <a href="https://lingomotions.com/", target="_blank">code to be released</a> /-->
                <a href="https://lingomotions.com/", target="_blank">project page</a> /
                <a href="https://youtu.be/-Uz6lGLdTy4", target="_blank">demo video</a> /
                <a shape="rect" href="javascript:togglebib('lingo')" class="togglebib">bibtex</a>

                <p align="justify"> <i id="lingo_abs">Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and Human-Object Interaction (HOI), presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured (MoCap) dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.</i></p>
                <pre xml:space="preserve"><bibtex>@misc{jiang2024autonomouscharactersceneinteractionsynthesis,
      title={Autonomous Character-Scene Interaction Synthesis from Text Instruction},
      author={Nan Jiang and Zimo He and Zi Wang and Hongjie Li and Yixin Chen and Siyuan Huang and Yixin Zhu},
      year={2024},
      eprint={2410.03187},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.03187},
}
</bibtex></pre>
                <p>
                   This paper introduces a framework for synthesizing multi-stage scene-aware interaction motions, and a comprehensive language-annotated MoCap dataset (LINGO).
                </p>
              </div>
            </td>
          </tr>


          <tr>
            <td width="33%" valign="top"><a href="images/trumans/cvpr.gif"><img src="images/trumans/cvpr.gif" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2403.08629.pdf", target="_blank" id="">
              <heading>Scaling Up Dynamic Human-Scene Interaction Modeling</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
              <strong>Nan Jiang*</strong>,
              <a href="https://zhiyuan-zhang0206.github.io/", target="_blank">Zhiyuan Zhang*</a>,
              <a href="https://pku.ai/author/hongjie-li/", target="_blank">Hongjie Li</a>,
              <a href="https://shirleymaxx.github.io/", target="_blank">Xiaoxuan Ma</a>,
              <a href="https://silvester.wang/", target="_blank">Zan Wang</a>,
              <a href="https://yixchen.github.io/", target="_blank">Yixin Chen</a>,
              <a href="https://tengyu.ai/", target="_blank">Tengyu Liu</a>,
              <a href="https://yzhu.io/", target="_blank">Yixin Zhu</a>,
              <a href="https://siyuanhuang.com/", target="_blank">Siyuan Huang</a>
              <br>
              <em>CVPR</em>, 2024 <font color="#9900FF">(Highlight)</font>
              <br></p>
              <div class="paper" id="trumans">
                <a onmouseover="showblock('trumans_abs')" onmouseout="hideblock('trumans_abs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2403.08629.pdf", target="_blank">paper</a> /
                <a href="https://github.com/jnnan/trumans_utils", target="_blank">code</a> /
                <a href="https://github.com/jnnan/trumans_utils", target="_blank">dataset</a> /
                <a href="https://jnnan.github.io/trumans/", target="_blank">project page</a> /
                <a shape="rect" href="javascript:togglebib('trumans')" class="togglebib">bibtex</a>
                    
                <p align="justify"> <i id="trumans_abs">Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction (HSI) modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{jiang2024scaling,
  title={Scaling up dynamic human-scene interaction modeling},
  author={Jiang, Nan and Zhang, Zhiyuan and Li, Hongjie and Ma, Xiaoxuan and Wang, Zan and Chen, Yixin and Liu, Tengyu and Zhu, Yixin and Huang, Siyuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1737--1747},
  year={2024}
}</bibtex></pre>
                <p>
                  We introduce TRUMANS, a large-scale MoCap dataset, featuring the most extensive motion-captured human-scene interactions. We further propose a novel approach for the generation of human-scene interaction sequences with arbitrary length.
                </p>
              </div>
            </td>
          </tr>


          <tr>
            <td width="33%" valign="top"><a href="images/phyrecon/teaser.png"><img src="images/phyrecon/teaser.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2404.16666", target="_blank" id="">
              <heading>PhyRecon: Physically Plausible Neural Scene Reconstruction</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
              <a href="https://dali-jack.github.io/Junfeng-Ni", target="_blank">Junfeng Ni*</a>,
              <a href="https://yixchen.github.io/", target="_blank">Yixin Chen*</a>,
              <a href="https://phyrecon.github.io/", target="_blank">Bohan Jing</a>,
              <strong>Nan Jiang</strong>,
              <a href="https://binwangbfa.github.io/", target="_blank">Bin Wang</a>,
              <a href="https://daibopku.github.io/daibo/", target="_blank">Bo Dai</a>,
              <a href="https://xiaoyao-li.github.io/", target="_blank">Puhao Li</a>,
              <a href="https://yzhu.io/", target="_blank">Yixin Zhu</a>,
              <a href="https://zhusongchun.net/", target="_blank">Songchun Zhu</a>,
              <a href="https://siyuanhuang.com/", target="_blank">Siyuan Huang</a>
              <br>
              <em>NeurIPS</em>, 2024
              <br></p>
              <div class="paper" id="phyrecon">
                <a onmouseover="showblock('phyrecon_abs')" onmouseout="hideblock('phyrecon_abs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2404.16666" target="_blank">paper</a> /
                <a href="https://github.com/PhyRecon/PhyRecon", target="_blank">code</a> /
                <a href="https://phyrecon.github.io/", target="_blank">project page</a> /
                <a shape="rect" href="javascript:togglebib('phyrecon')" class="togglebib">bibtex</a>

                <p align="justify"> <i id="phyrecon_abs">Neural implicit representations have gained popularity in multi-view 3D reconstruction. However, most previous work struggles to yield physically plausible results, limiting their utility in domains requiring rigorous physical accuracy, such as embodied AI and robotics. This lack of plausibility stems from the absence of physics modeling in existing methods and their inability to recover intricate geometrical structures. In this paper, we introduce PhyRecon, the first approach to leverage both differentiable rendering and differentiable physics simulation to learn implicit surface representations. PhyRecon features a novel differentiable particle-based physical simulator built on neural implicit representations. Central to this design is an efficient transformation between SDF-based implicit representations and explicit surface points via our proposed Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses. Additionally, PhyRecon models both rendering and physical uncertainty to identify and compensate for inconsistent and inaccurate monocular geometric priors. This physical uncertainty further facilitates a novel physics-guided pixel sampling to enhance the learning of slender structures. By integrating these techniques, our model supports differentiable joint modeling of appearance, geometry, and physics. Extensive experiments demonstrate that PhyRecon significantly outperforms all state-of-the-art methods. Our results also exhibit superior physical stability in physical simulators, with at least a 40% improvement across all datasets, paving the way for future physics-based applications.</i></p>
                <pre xml:space="preserve"><bibtex>@article{ni2024phyrecon,
  title={PhyRecon: Physically Plausible Neural Scene Reconstruction},
  author={Junfeng Ni and Yixin Chen and Bohan Jing and Nan Jiang and Bin Wang and Bo Dai and Puhao Li and Yixin Zhu and Song-Chun Zhu and Siyuan Huang},
  journal={arXiv preprint arXiv:2404.16666},
  year={2024}
}
}</bibtex></pre>
                <p>
                   We introduce PhyRecon, the first approach to leverage both differentiable rendering and differentiable physics simulation to learn implicit surface representations.
                </p>
              </div>
            </td>
          </tr>

          <tr>
            <td width="33%" valign="top"><a href="images/chairs/teaser.png"><img src="images/chairs/teaser.png" width="100%" height="120%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2212.10621.pdf", target="_blank" id="">
              <heading>Full-Body Articulated Human-Object Interaction</b></heading></a>
              <br>
              <strong>Nan Jiang*</strong>,
              <a href="https://tengyu.ai/", target="_blank">Tengyu Liu*</a>,
              <a href="https://pku.ai/author/zhexuan-cao/", target="_blank">Zhexuan Cao</a>,
              <a href="https://jiemingcui.github.io/", target="_blank">Jieming Cui</a>,
              <a href="https://zhiyuan-zhang0206.github.io/", target="_blank">Zhiyuan Zhang</a>,
              <a href="https://yixchen.github.io/", target="_blank">Yixin Chen</a>,
              <a href="https://hughw19.github.io/", target="_blank">He Wang</a>,
              <a href="https://yzhu.io/", target="_blank">Yixin Zhu</a>,
              <a href="https://siyuanhuang.com/", target="_blank">Siyuan Huang</a>
              <br>
              <em>ICCV</em>, 2023
              <br></p>
              <div class="paper" id="chairs">
                <a onmouseover="showblock('chairs_abs')" onmouseout="hideblock('chairs_abs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2212.10621.pdf" target="_blank">paper</a> /
                <a href="https://github.com/jnnan/chairs", target="_blank">code</a> /
                <a href="https://forms.gle/t4SjmJS4RPx7AFvFA", target="_blank">dataset</a> /
                <a href="https://jnnan.github.io/project/chairs/", target="_blank">project page</a> /
                <a shape="rect" href="javascript:togglebib('chairs')" class="togglebib">bibtex</a>
                    
                <p align="justify"> <i id="chairs_abs">Fine-grained capturing of 3D HOI boosts human activity understanding and facilitates downstream visual tasks, including action recognition, holistic scene reconstruction, and human motion synthesis. Despite its significance, existing works mostly assume that humans interact with rigid objects using only a few body parts, limiting their scope. In this paper, we address the challenging problem of f-AHOI, wherein the whole human bodies interact with articulated objects, whose parts are connected by movable joints. We present CHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hours of versatile interactions between 46 participants and 74 articulated and rigid sittable objects. CHAIRS provides 3D meshes of both humans and articulated objects during the entire interactive process, as well as realistic and physically plausible full-body interactions. We show the value of CHAIRS with object pose estimation. By learning the geometrical relationships in HOI, we devise the very first model that leverage human pose estimation to tackle the estimation of articulated object poses and shapes during whole-body interactions. Given an image and an estimated human pose, our model first reconstructs the pose and shape of the object, then optimizes the reconstruction according to a learned interaction prior. Under both evaluation settings (e.g., with or without the knowledge of objects' geometries/structures), our model significantly outperforms baselines. We hope CHAIRS will promote the community towards finer-grained interaction understanding. We will make the data/code publicly available.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{jiang2023full,
  title={Full-body articulated human-object interaction},
  author={Jiang, Nan and Liu, Tengyu and Cao, Zhexuan and Cui, Jieming and Zhang, Zhiyuan and Chen, Yixin and Wang, He and Zhu, Yixin and Huang, Siyuan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9365--9376},
  year={2023}
}</bibtex></pre>
                <p>
                  We present CHAIRS, a large-scale motion-captured dataset featuring full-body articulated human-object interaction. We devise the very first model that leverage human pose estimation to tackle the reconstruction of articulated object poses and shapes.
                </p>
              </div>
            </td>
          </tr> 
          
          
          <tr>
            <td width="33%" valign="top"><a href="images/ssr/teaser.png"><img src="images/ssr/teaser.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="http://arxiv.org/abs/2311.00457", target="_blank" id="">
              <heading>Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture</b></heading></a>&nbsp;
              <br>
              <a href="https://yixchen.github.io/", target="_blank">Yixin Chen*</a>,
              <a href="https://dali-jack.github.io/Junfeng-Ni", target="_blank">Junfeng Ni*</a>,
              <strong>Nan Jiang</strong>,
              <a href="https://dali-jack.github.io/SSR/", target="_blank">Yaowei Zhang</a>,
              <a href="https://yzhu.io/", target="_blank">Yixin Zhu</a>,
              <a href="https://siyuanhuang.com/", target="_blank">Siyuan Huang</a>
              <br>
              <em>3DV</em>, 2024
              <br></p>
              <div class="paper" id="ssr">
                <a onmouseover="showblock('ssr_abs')" onmouseout="hideblock('ssr_abs')">abstract</a> /
                <a href="https://arxiv.org/pdf/2404.16666" target="_blank">paper</a> /
                <a href="https://github.com/DaLi-Jack/SSR-code", target="_blank">code</a> /
                <a href="https://dali-jack.github.io/SSR/", target="_blank">project page</a> /
                <a shape="rect" href="javascript:togglebib('ssr')" class="togglebib">bibtex</a>
                    
                <p align="justify"> <i id="ssr_abs">Reconstructing detailed 3D scenes from single-view images remains a challenging task due to limitations in existing approaches, which primarily focus on geometric shape recovery, overlooking object appearances and fine shape details. To address these challenges, we propose a novel framework for simultaneous high-fidelity recovery of object shapes and textures from single-view images. Our approach utilizes SSR, Single-view neural implicit Shape and Radiance field representations, leveraging explicit 3D shape supervision and volume rendering of color, depth, and surface normal images. To overcome shape-appearance ambiguity under partial observations, we introduce a two-stage learning curriculum that incorporates both 3D and 2D supervisions. A distinctive feature of our framework is its ability to generate fine-grained textured meshes while seamlessly integrating rendering capabilities into the single-view 3D reconstruction model. This integration enables not only improved textured 3D object reconstruction by 27.7% and 11.6% on the 3D-FRONT and Pix3D datasets, respectively, but also supports the rendering of images from novel viewpoints. Beyond individual objects, our approach facilitates composing object-level representations into flexible scene representations, thereby enabling applications such as holistic scene understanding and 3D scene editing.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{chen2023ssr,
               title={Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture},
               author={Chen, Yixin and Ni, Junfeng and Jiang, Nan and Zhang, Yaowei and Zhu, Yixin and Huang, Siyuan},
               booktitle=ThreeDV,
               year={2024}
}</bibtex></pre>
                <p>
                   we propose a novel framework for simultaneous high-fidelity recovery of object shapes and textures from single-view images. We introduce a two-stage learning curriculum that incorporates both 3D and 2D supervisions.
                </p>
              </div>
            </td>
          </tr> 

        </table>

        

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('trumans_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('phyrecon_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('chairs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ssr_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lingo_abs');
</script>


<script>
    const bibTex_list = document.querySelectorAll("bibtex");

    bibTex_list.forEach((bibTex) => {
        bibTex.onclick = function() {
          document.execCommand("copy");
      }

      bibTex.addEventListener("copy", function(event) {
      event.preventDefault();
      if (event.clipboardData) {
          event.clipboardData.setData("text/plain", bibTex.innerText);
          alert("Citation copied to clipboard!");
      }
      });
    });

</script>

</body>

</html>
